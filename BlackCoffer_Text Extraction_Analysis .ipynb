{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1877bdfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count is  162631\n",
      "Positive score are: 2497\n",
      "Negative score are: 1622\n",
      "Polarity_Score is: 0.21243020145364644\n",
      "Positive_word_proportion is 0.015353776340304125\n",
      "Positive_word_proportion is 0.009961200509128026\n",
      "Average length of words are: 5.13178247734139\n",
      "Number of Complex Words: 258991\n",
      "The percentage of complex words are: 1.5925069636170226\n",
      "The Fog index is : 2.6897157763833652\n",
      "Constraining score is:  74\n",
      "Constraining proportion is: 0.00045501780103424317\n",
      "Uncertainty score is: 62\n",
      "Uncertainty proportion is: 0.00038123113059625776\n",
      "Link1_2ndcell.txt\n"
     ]
    }
   ],
   "source": [
    "#### Import required package\n",
    "\n",
    "import nltk\n",
    "# nltk.download() #for first time just download all-nltk\n",
    "import nltk.corpus\n",
    "import pandas as pd\n",
    "import regex\n",
    "import re \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "SIA = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "## User defined function to convert list to a string\n",
    "def ListToString(l):         #function name is 'ListToString' with list 'l' as input\n",
    "    string = ''              #empty string\n",
    "    return(string.join(l))   #Join list as string\n",
    "\n",
    "\n",
    "## reading and convert to lower text\n",
    "with open(\"Link1_2ndcell.txt\",'r') as fd:                   #open the .txt file\n",
    "    lines_with_endchar = fd.read().splitlines()             #read text line by line\n",
    "    TEXT_1 = ListToString(lines_with_endchar)         #convert whole text into lower case\n",
    "    TEXT_2 = TEXT_1.lower()\n",
    "\n",
    "\n",
    "## Tokenize the word\n",
    "from nltk.tokenize import word_tokenize\n",
    "TEXT_3 = word_tokenize(TEXT_2)\n",
    "\n",
    "\n",
    "## Find the URL's in the text\n",
    "def Find_URL(string):\n",
    "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?<<>>\"\"'']))\"\n",
    "    url=re.findall(regex,string)\n",
    "    TEXT_URLs =[x for x in url]\n",
    "    print(\"URL's are:\",url)\n",
    "\n",
    "    \n",
    "#remove numbers  from the list \n",
    "TEXT_4 = [x for x in TEXT_3 if x.isnumeric()==False]\n",
    "\n",
    "\n",
    "#### Remove punctuation\n",
    "punctuation_marks='''!~[]()<>{}-?&$!,--#%^:;@``\"\"''================================================================================|\n",
    "'''\n",
    "TEXT_5 =[]\n",
    "for p in TEXT_4:\n",
    "    if p not in punctuation_marks:\n",
    "        TEXT_5.append(p)\n",
    "\n",
    "        \n",
    "## remove Stop words\n",
    "english_stop_words = stopwords.words('english')\n",
    "def remove_stop_words(corpus):\n",
    "    removed_stop_words = []\n",
    "    for review in corpus:\n",
    "        removed_stop_words.append(' '.join([word for word in review.split() \n",
    "                      if word not in english_stop_words]))\n",
    "    return removed_stop_words\n",
    "TEXT_6 = remove_stop_words(list(TEXT_5))\n",
    "#print(TEXT_6)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "unwanted = unwanted = [TEXT_6[6],TEXT_6[7],TEXT_6[8],TEXT_6[11],TEXT_6[12],TEXT_6[14],TEXT_6[16]]\n",
    "for word in unwanted:\n",
    "    TEXT_6.remove(word)\n",
    "\n",
    "\n",
    "#### word Count\n",
    "\n",
    "def Word_Count(text):\n",
    "    count = 0\n",
    "    for word in text:\n",
    "        count = count +1 \n",
    "    return count\n",
    "\n",
    "No_of_Word_Count =Word_Count(TEXT_6)\n",
    "print(\"Word count is \",No_of_Word_Count)\n",
    "from textblob import TextBlob\n",
    "TextBlob(ListToString(TEXT_6)).sentiment\n",
    "\n",
    "def find_polarity(TEXT_6):\n",
    "    return TextBlob(TEXT_6).sentiment.polarity\n",
    "Polarity = []\n",
    "for i in TEXT_6:\n",
    "    Polarity.append(find_polarity(i))\n",
    "Positive_polarity_INDEX = [x for x in Polarity if x>0]\n",
    "Negative_polarity_INDEX = [x for x in Polarity if x<0]\n",
    "\n",
    "Positive_Score = len(Positive_polarity_INDEX)\n",
    "Negative_Score = len(Negative_polarity_INDEX)\n",
    "print(\"Positive score are:\",len(Positive_polarity_INDEX))\n",
    "print(\"Negative score are:\",len(Negative_polarity_INDEX))\n",
    "\n",
    "Polarity_Score= (Positive_Score - Negative_Score)/((Positive_Score + Negative_Score) + 0.000001)\n",
    "print(\"Polarity_Score is:\",Polarity_Score)\n",
    "#print(\"Polarity Score is\",Positive_Score - Negative_Score/((Positive_Score + Negative_Score)+0.000001))\n",
    "\n",
    "Positive_word_proportion = len(Positive_polarity_INDEX)/No_of_Word_Count\n",
    "\n",
    "Negative_word_proportion = (len(Negative_polarity_INDEX)-2)/No_of_Word_Count\n",
    "print(\"Positive_word_proportion is\",Positive_word_proportion)\n",
    "print(\"Positive_word_proportion is\",Negative_word_proportion)\n",
    "#### Average length of words\n",
    "\n",
    "str_TEXT_6 = str(TEXT_6)\n",
    "sentence = str_TEXT_6.split(\".\")\n",
    "avg_len = sum(len(x.split()) for x in sentence)/len(sentence)\n",
    "print(\"Average length of words are:\",avg_len)\n",
    "\n",
    "# WORDS with Syllable\n",
    "\n",
    "def syllable_words(text):\n",
    "    syllable_word=[]\n",
    "    for word in text:\n",
    "        for letter in word:\n",
    "            if letter in {\"a\",\"e\",\"i\",\"o\",\"u\"}:\n",
    "                syllable_word.append(word)\n",
    "                break\n",
    "    return(len(syllable_word))\n",
    "#print(\"Total no of Syllables are:\",count)\n",
    "#print(\"This Text of {} cleaned words have {} syllable words\".format(len(TEXT_6),syllable_words(TEXT_6)))\n",
    "\n",
    "\n",
    "\n",
    "### LETTERS with Syllable\n",
    "\n",
    "def syllable_letter(text):\n",
    "    syllable_letter=[]\n",
    "    for word in text:\n",
    "        for letter in word:\n",
    "            \n",
    "            if letter in {\"a\",\"e\",\"i\",\"o\",\"u\"}:\n",
    "                syllable_letter.append(letter)\n",
    "                \n",
    "    return(len(syllable_letter))\n",
    "#print(\"Total no of Syllables are:\",count)\n",
    "#print(\"This Text of {} cleaned words have {} syllable letters\".format(len(TEXT_6),syllable_letter(TEXT_6)))\n",
    "\n",
    "\n",
    "\n",
    "## Complex Words (words with syllable >=2)\n",
    "def complex_word_count(text):\n",
    "    COUNT_LIST =[]\n",
    "    for word in text:\n",
    "        count = 0\n",
    "\n",
    "        count_list =[]\n",
    "        for letter in word:\n",
    "            if letter in {\"a\",\"e\",\"i\",\"o\",\"u\"}:\n",
    "                count = count +1\n",
    "                count_list.append(count)\n",
    "                COUNT_LIST.append(len(count_list) )  \n",
    "    \n",
    "    #return((COUNT_LIST))\n",
    "    all_indices = []\n",
    "    w = 0\n",
    "    for i in range(0,len(COUNT_LIST)):\n",
    "        w = w+1\n",
    "        if COUNT_LIST[i] >= 2:\n",
    "            all_indices.append(i)\n",
    "    return w\n",
    "#print(\"Total no of Syllables are:\",count)\n",
    "#print(\"This Text of {} cleaned words have {} syllable letters\".format(len(TEXT_6),complex_word_count(TEXT_6)))\n",
    "\n",
    "\n",
    "No_of_COMPLEX_WORDS=complex_word_count(TEXT_6)\n",
    "print(\"Number of Complex Words:\",No_of_COMPLEX_WORDS)\n",
    "\n",
    "### percentage of complex words\n",
    "#Percentage of Complex words = the number of complex words / the number of words\n",
    "\n",
    "Percentage_of_Complex_words = No_of_COMPLEX_WORDS / No_of_Word_Count\n",
    "print(\"The percentage of complex words are:\",Percentage_of_Complex_words)\n",
    "\n",
    "\n",
    "\n",
    "### Fog index\n",
    "#Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
    "Fog_Index = 0.4 * (avg_len + Percentage_of_Complex_words)\n",
    "print(\"The Fog index is :\",Fog_Index)\n",
    "\n",
    "### Frequency analysis\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()\n",
    "for word in list(TEXT_6):\n",
    "    fdist[word.lower()]+=1 #Converting all words to lower case and then counting probability \n",
    "fdist\n",
    "\n",
    "constraining_Sheet = pd.read_excel(\"constraining_dictionary (1).xlsx\")\n",
    "constraining = constraining_Sheet['Word']\n",
    "constraining_Sheet = constraining_Sheet.drop(\"Word\",axis =1)\n",
    "constraining_lower = []\n",
    "for word in constraining:\n",
    "    word_lower = word.lower()\n",
    "    constraining_lower.append(word_lower)    \n",
    "constraining_Sheet['word'] = constraining_lower\n",
    "C_ing = constraining_Sheet['word']\n",
    "\n",
    "C_ING = [x for x in C_ing if x in TEXT_6]\n",
    "constraining_word_count = len(C_ING)\n",
    "print(\"Constraining score is: \",constraining_word_count)\n",
    "\n",
    "constraining_proportion = constraining_word_count/No_of_Word_Count\n",
    "print(\"Constraining proportion is:\",constraining_proportion)\n",
    "\n",
    "\n",
    "\n",
    "uncertainty_Sheet = pd.read_excel(\"uncertainty_dictionary.xlsx\")\n",
    "\n",
    "uncertainty = uncertainty_Sheet['Word']\n",
    "uncertainty_Sheet = uncertainty_Sheet.drop(\"Word\",axis =1)\n",
    "uncertainty_lower = []\n",
    "for word in uncertainty:\n",
    "    word_lower = word.lower()\n",
    "    uncertainty_lower.append(word_lower)    \n",
    "uncertainty_Sheet['word'] = uncertainty_lower\n",
    "UN_ty = uncertainty_Sheet['word']\n",
    "\n",
    "UN_TY = [x for x in UN_ty if x in TEXT_6]\n",
    "uncertainty_word_count = len(UN_TY)\n",
    "print(\"Uncertainty score is:\", uncertainty_word_count)\n",
    "\n",
    "uncertainty_proportion = uncertainty_word_count/No_of_Word_Count\n",
    "print(\"Uncertainty proportion is:\",uncertainty_proportion)\n",
    "print(\"Link1_2ndcell.txt\")#55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0ae0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
